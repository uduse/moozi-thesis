\chapter{Experiments}
\label{sec:exp}
We run the \textit{MooZi} system on both video games and board games to demonstrate MooZi's ability to learn and perform in both domains.
We use MinAtar environments (reviewed in Section \ref{sec:min_atar}) as video game environments, and Breakthrough as a board game environment.
We compare MooZi's performance in MinAtar environments with published result from Gymnax \cite{GymnaxJAXbasedReinforcement_RobertTjarkoLange_2022} and MinAtar \cite{MinAtarAtariInspiredTestbed_Young.Tian_2019}.
We evaluate MooZi's performance in Breakthrough by competing MooZi models with different amount of training.
We analyze MooZi's behavior in these environments and discuss the behavior of the system during training.

\section{Experiment Setup}
\subsection{Basic Configuration}
For all of our experiments, we use MooZi with unrolled steps $K = 5$, history length $L = 4$, and bootstrap steps $N = 10$.
We use a discount of $0.997$ and a support $Z$ from the interval  $[-30, 30]$ ($|Z| = 61$).
MuZero by \citeauthor{MasteringAtariGo_Schrittwieser.Antonoglou.ea_2020} only used transformation $\Phi$ in Atari environments \cite{MasteringAtariGo_Schrittwieser.Antonoglou.ea_2020}, but we discovered the same support works well for both MinAtar environments and \textit{Breakthrough}, so we do not make such distinction.
We run all experiments on a single computer with Intel Xeon CPUs (72 $\times$ 2.3 GHz), Nvidia Tesla V100 GPUs (8 $\times$ 32 GB), and 500 Gigabytes of system memory.
Each run roughly consume 50\% of available computation resources of the computer, and we usually run two instances of the system at the same time.

\subsection{Neural Network Configurations}
We use the residual-blocks-based variant (see Section \ref{sec:nn}) of the network for all of our experiments.

\subsubsection{Residual Block}
We follow the residual block definition by \citeauthor{DeepResidualLearning_He.Zhang.ea_2016} \cite{DeepResidualLearning_He.Zhang.ea_2016}.
The computation of one residual block is computed as follows:
\begin{itemize}
    \item input $x$
    \item save a copy of $x$ to $x'$
    \item apply a 2-D padded convolution on $x$, with kernel size 3 by 3, same channels
    \item apply batch normalization on $x$
    \item apply ReLU activation on $x$
    \item apply a 2-D padded convolution on $x$, with kernel size 3 by 3, same channels
    \item apply batch normalization on $x$
    \item add $x'$ to $x$
    \item apply ReLU activation on $x$
\end{itemize}

\subsubsection{The Representation Function Network}
The representation function $\mathbf{x} = h$ is computed as follows:
\begin{itemize}
    \item input a stacked history $\psi$ of shape $(H, W, C_h)$
    \item apply a 2-D padded convolution on $\psi$, with kernel size 1 by 1 and 32 channels
    \item apply 6 residual blocks with 32 channels on $\psi$
    \item output the hidden state $\mathbf{x}$ of shape $(H, W, 32)$
\end{itemize}

\subsubsection{The Prediction Function Network}
The prediction function $\mathbf{p}, v = f(\mathbf{x})$ is computed as follows:
\begin{itemize}
    \item input hidden state $\mathbf{x}$ of shape $(H, W, 32)$
    \item apply 1 residual block with 32 channels on $\mathbf{x}$
    \item flatten $\mathbf{x}$, now shape $(H * W * 32)$
    \item apply 1 dense layer with output size of 128 on flattened $\mathbf{x}$ to obtain the value head $\mathbf{x}_v$, now shape $(128)$
    \item apply batch normalization and ReLU activation on $\mathbf{x}_v$
    \item apply 1 dense layer with output size of $|Z|$ on $\mathbf{x}_v$, now shape $(1)$
    \item output the value head $\mathbf{x}_v$ as the value prediction $v$
    \item apply 1 dense layer with output size of 128 on flattened $\mathbf{x}$ to obtain the policy head $\mathbf{x}_p$, now shape $(128)$
    \item apply batch normalization and ReLU activation on $\mathbf{x}_p$
    \item apply 1 dense layer with output size equals to the action space size on $\mathbf{x}_p$, now shape $(|\mathcal{A}^a|)$
    \item output the policy head as the policy prediction $\mathbf{p}$
\end{itemize}

\subsubsection{The Dynamics Function Network}
The dynamics function $\mathbf{x}, \hat{r} = g(\mathbf{x})$ is computed as follows:
\begin{itemize}
    \item input hidden state $\mathbf{x}$ of shape $(H, W, 32)$, action $a$ as an integer
    \item encode $a$ as action planes of shape $(H, W, |\mathcal{A}^a|)$ (same procedure as in Section \ref{sec:history_stacking})
    \item stack $\mathbf{x}$ on top of the encoded action, now shape $(H, W, 32 + |\mathcal{A}^a|)$

    \item apply a 2-D padded convolution on $\psi$, with kernel size 1 by 1, 32 channels, now shape $(H, W, 32)$
    \item apply 1 residual block with 32 channels on $\mathbf{x}$ to obtain the hidden state head $\mathbf{x}_s$
    \item apply 1 residual block with 32 channels on the hidden state head $\mathbf{x}_s$
    \item output the hidden state head $\mathbf{x}_s$ as the next hidden state $\mathbf{x}'$

    \item apply 1 dense layer with output size of 128 on $\mathbf{x}$ to obtain the reward head $\mathbf{x}_r$, now shape of $(128)$
    \item apply batch normalization and ReLU activation on $\mathbf{x}_r$
    \item apply 1 dense layer with output size of $|Z|$ on $\mathbf{x}_r$, now shape of $(|Z|)$
    \item output reward head $\mathbf{x}_r$ as the reward prediction $\hat{r}$
\end{itemize}
\citeauthor{MasteringAtariGames_Ye.Liu.ea_2021} discovered a small dynamics function network is sufficient for Atari environments, and our MinAtar experiments also use a small dynamics function network with only one residual block as the trunk.
However, we noticed such a small dynamics function network isn't sufficient to fully learn the environment dynamics in \textit{Breakthrough}.
As a result, we use a larger dynamics function network with 6 residual blocks for \textit{Breakthrough} experiments in Section \ref{sec:exp:breakthrough}.

\subsubsection{The Projection Function Network}
The projection function $\mathbf{x} = \varrho(\mathbf{x})$ is simply one residual block (see Section \ref{sec:nn}).


\subsubsection{Network Training}
In the loss function described in Section \ref{sec:loss}, we use parameters $c^v = 0.25$, $c^s = 2.0$, and $c^{L_2} = \num{1.0e-4}$.
We use a batch size of 1024, a learning rate of \num{1.0e-2}, and a global norm clipping of \num{5.0}.
We perform gradient updates with a number of samples that is four times the number of step samples generated in each training step.
For example, if we have 30 training workers, each with 16 environments, and each performs 100 environment steps per training step, then the total number of step samples generated is $30 * 16 * 100 = 48000$.
This means that we update the gradient using $4 * 48000 = 192000$ sampled training targets from the replay buffer.
That is $\frac{192000}{1024} \approx 188$ gradient updates from mini-batches of size 1024 per training step.
% We use a target network (see Section \ref{sec:dqn} and \cite{PlayingAtariDeep_Mnih.Kavukcuoglu.ea_2013}) to smooth gradient updates, and we update the target network every other 500 gradient updates.

\subsection{Planner Configurations} \label{sec:planner_config}
\begin{table}
    \begin{tabular}{@{}|l|l|l|l|l|l|@{}}
        \toprule
        Worker    & $c_1$ & $c_2$ & Temperature & Dirichlet Noise & Simulations \\ \midrule
        Training  & 2.25  & 19652 & 1.0         & 0.2             & 25          \\ \midrule
        Reanalyze & 1.75  & 19652 & -           & 0.2             & 50          \\ \midrule
        Testing   & 1.75  & 19652 & 0.25        & 0.1             & 40          \\ \bottomrule
    \end{tabular}
    \captionsetup{width=\linewidth, justification=raggedright}
    \caption[Planner Configuration]{\textbf{Planner configurations.}}
    \label{tb:planner_args}
\end{table}

Table \ref{tb:planner_args} shows the configurations of planners from different type of workers.
Reanalyze workers do not sample actions to act so the temperature parameter does not affect their behavior.
$c_1$ is the exploration constant in the PUCT formula form \ref{sec:muzero}.
A greater $c_1$ favors the less visited actions more.
$c_2$ is also an exploration constant in the PUCT formula.
We use the same value as \cite{MasteringAtariGo_Schrittwieser.Antonoglou.ea_2020} because we find it sufficient to tune $c_1$ to balance exploration.
The \textit{temperature} controls how action is selected from the distribution of action visit counts from the root nodes.
A temperature of 0 selects most visited action at the root node.
A temperature of $\infty$ means random action selections.
The \textit{dirichlet noise} controls the exploration noise added to the actions at the root nodes.
A greater dirichlet noise adds more prior probability to less explored actions.
The \textit{simulations} parameter is the number of simulations the planner performs at each timestep (see Section \ref{sec:mcts}).
The training workers favor exploration and generate data quickly with fewer simulations.
The testing workers favor exploitation and spend more time on simulations to get better average return.
The reanalyze workers do not need to interact with the environment so they spend more time performing simulations.

\subsection{Driver Configuration} \label{sec:exp:driver}
We use 30 training workers, each with 16 copies of the environment.
For every training step, each training worker performs 100 environment steps.
The \textit{Freeway} environment has much longer episodes, so each training worker uses only 2 environments and performs 2500 environment steps per training step.
We use 1 testing worker with one copy of the environment.
For every 10 training steps, the testing worker collects 10 trajectories and logs the average return of the trajectories.
We only use the reanalyze worker for the experiments in Section \ref{sec:exp:re}.
All workers sync with the latest neural network parameters every 10 training steps.

\section{MooZi vs PPO, in MinAtar Environments} \label{sec:exp:minatar}
We run MooZi using four environments in MinAtar, including \textit{Breakout}, \textit{Space Invaders}, \textit{Freeway}, and \textit{Asterix} \cite{MinAtarAtariInspiredTestbed_Young.Tian_2019}.
All environments produce frames of resolution $10 \times 10$, with four to seven environment channels $C_e$.
In \textit{Breakout}, the player moves a paddle left or right on the bottom of the screen to bounce a ball to hit the bricks.
The reward is $+1$ for each brick broken and $0$ in all other situations.
The game ends when the paddle fails to catch the ball.
In \textit{Space Invaders}, the player controls a cannon that can move left, move right, or fire the cannon.
A cluster of enemies moves across the screen and they fire at the player.
The reward is $+1$ for each enemy hit by the player and $0$ in all other situations.
The game ends when the player is hit by an enemy's bullet.
In \textit{Freeway}, the player starts at the bottom of the screen and can move up or down once every three frames to travel across a road with traffic, or stay in place.
Cars spawn randomly and travel horizontally across the screen.
When they hit the player the player is moved back to its starting position.
The reward is $+1$ for each time the player successfully travels across the whole road and $0$ in all other situations.
The game ends after 2500 timesteps.
In \textit{Asterix}, the player moves in four directions, and enemies and treasures spawn randomly on the edges.
The reward is $+1$ if the player obtains a treasure and $0$ in all other situations.
The game ends when the player bumps into an enemy.
Enemies have different speed indicated by the color of their tail.
Figure \ref{fig:minatar} shows a screen shot of the MinAtar environments.
MuZero is reported to have much better performance in more deterministic environments \cite{VectorQuantizedModels_Ozair.Li.ea_2021}.
To compare MooZi in more deterministic environments, we compare with the results reported by Gymnax, in which algorithms are evaluated in environments with no sticky action.
We discuss this difference in section \ref{sec:sticky_minatar}.
Gymnax benchmarked the performance of PPO with three sets of hyper-parameters in each of these four games \cite{GymnaxJAXbasedReinforcement_RobertTjarkoLange_2022,ProximalPolicyOptimization_Schulman.Wolski.ea_2017}.
We compare our results with the best performing PPO in each of these games.
The average return uses the planner setting of a test worker in Section \ref{sec:planner_config}.
Figure \ref{fig:experiments_minatar_moozi_vs_ppo} shows the result comparisons.
In \textit{Breakout}, MooZi obtained a near-optimal strategy and the paddle almost never fails to catch the ball.
There is no default step limits in the environments so we cap the return at 100.
Similarly in \textit{Space Invaders}, MooZi obtained a near-optimal strategy and we capt the return at 300.
In \textit{Freeway}, MooZi also obtained a near-optimal strategy.
Since episode in this environment is much longer than other environments, MooZi cycles through less training steps and save less checkpoints.
Hence MooZi has much less data points in the line plot for this environment.
In \textit{Asterix}, both MooZi and PPO do not obtain an optimal strategy, but MooZi have twice as much average return as PPO.
In all environments, a single run takes about 10 hours.
Gymnax does not provide their runtime statistics, but we presume their result is at least an order of magnitude faster that ours since they do not use planning in acting.

\includeimage[0.6]{minatar}{\textbf{MinAtar Environments, from \cite{MinAtarAtariInspiredTestbed_Young.Tian_2019}.}}{}
\includeimage[0.6]{experiments_minatar_moozi_vs_ppo}{\textbf{MinAtar games. MooZi vs PPO.}}{}

\section{Sticky Actions in MinAtar} \label{sec:sticky_minatar}
\includeimage[0.6]{experiments_breakout_w_wo_sticky_actions}{
    \textbf{MooZi vs PPO vs AC in \textit{Breakout}. Plot of environment frames used in training (x-axis, in unit of \num{1.0e7} frames) vs average return (y-axis).}
}{}
MinAtar environments have a default sticky action probability of 10\%.
This means that in one out of ten timesteps, the environment uses the last taken action to step the environment instead of using the agent's output action.
For example, assume that at time $t$, the agent outputs action $a_t = \text{\textit{MoveLeft}}$ and moves to the left.
At time $t+1$, the agent outputs action $a_{t+1} = \text{\textit{MoveRight}}$.
However, if the environment applies the sticky action and overrides the action, then $a_{t+1} = \text{\textit{MoveLeft}}$, and the agent moves to the left even further.
The presence of a non-zero sticky action probability adds stochasticity to environments and changes the set of optimal polices.
For example, in \textit{Space Invaders}, if sticky action probability is 0, then the agent can move away from enemy bullets just in time, one frame before the agent is about to get hit.
However, with a sticky action probability of 10\%, moving away one frame in advance means there's a 10\% chance that the agent will die, and moving away two frames in advance means there's a $(10\% * 10\%) = 1\%$ chance that the agent will die two frames later.
We observe that a \moozi agent trained in \textit{Space Invaders} with a 10\% sticky action probability moves away from an enemy bullet right after the bullet becomes visible on the screen.
\citeauthor{MinAtarAtariInspiredTestbed_Young.Tian_2019} shipped the MinAtar environments with four algorithms including two variants of DQN and two variants of Actor-Critic (AC) method.
The MinAtar paper \cite{MinAtarAtariInspiredTestbed_Young.Tian_2019} only reports \textit{Breakout} results with a sticky action probability of 10\%.
Gymnax only reports \textit{Breakout} results without sticky action probability.
We compare with MinAtar and Gymnax in their respective testing environments using the same \moozi agent configuration.
Figure \ref{fig:experiments_breakout_w_wo_sticky_actions} shows the comparison.
We use the final average returns of PPO and AC reported in Gymnax and MinAtar's paper respectively.
In \textit{Breakout} with sticky actions, \moozi learns more slowly and the final average return is much lower.
In \textit{Breakout} without sticky actions, \moozi quickly learns a near-optimal policy and never fails to return the ball.
In both environments, \moozi out-performs the other algorithm.
\section{Testing Strength when Scaling the Search Budget in \textit{Asterix}}
We use the trained MooZi model in the \textit{Space Invaders} environment to evaluate its strength while varying the different number of simulations.
We only use model checkpoints from the first 3 million environment frames because after that the agent behaviors optimally even when using the prior policy to act.
For each of these checkpoints, we run a testing worker to collect 30 episodes and compute the average return of these episodes.
The testing worker uses the same planner configuration as the testing worker in Section \ref{sec:planner_config} except for the number of simulations.
Figure \ref{fig:experiments_space_invaders_vs_simulations} shows the result.
We observe that with a greater number of simulations, the agent tends to perform better.
With more training, the agent always perform better.
The difference of performance due to simulation count seems to be smaller than the difference due to training.
For example, with 2.5 million frames of training, acting using 64 simulations gives an average return around 70.
With 3 million frames of training, acting according to the prior (1 simulation) does even better and gives an average return around 75.
The prior policy, with a bit more training, quickly catches up to or even exceeds the deep search policy with less training.
This finding aligns with the analysis by \citeauthor{ROLEPLANNINGMODELBASED_Hamrick.Friesen.ea_2021} \cite{ROLEPLANNINGMODELBASED_Hamrick.Friesen.ea_2021}:
the planner contributes more to the algorithm by \textit{generating better data for training the model} rather than \textit{exploiting the model for better testing}.
\includeimage[1]{experiments_space_invaders_vs_simulations}{
    \textbf{Agent strength with different number of simulations.}
}{}


\section{Improving Sample Efficiency with Reanalyze in \textit{Asterix}} \label{sec:exp:re}

\includeimage{reana}{
    \textbf{
        \moozi with reanalyze workers in \textit{Asterix}.
    }}
{
    Plot of environment frames used in training (x-axis, in unit of \num{1.0e6} frames) vs average return (y-axis in log scale).
}

We run three sets of parameters of MooZi, each with three experiments in the \textit{Asterix} environment.
We configure training workers and reanalyze workers to generate the exact same amount of training targets for each training step.
We control the total number of generated training targets and network updates per training step by fixing the sum of number of training workers and reanalyze workers to 20.
The parameter set \textit{20 : 0} indicates 20 training workers and 0 reanalyze worker.
This set is the same as that used in Section \ref{sec:exp:minatar}.
The parameter set \textit{16 : 4} uses 16 training workers and 4 reanalyze workers.
Similarly, the parameter set \textit{12 : 8} uses 12 training workers and 8 reanalyze workers.
We also fix the total environments frames to be 10 million.
Parameter sets with more reanalyze workers take more time to run, as they consume environment frames budget more slowly.
Figure \ref{fig:reana} shows the performance of MooZi with Reanalyze workers.
% In the early stage of the training, runs with more reanalyze workers yield higher average returns using the same amount of environment frames.
% However, this effect diminish as the training progress and completely vanish in the late stage of the training.
The parameter set with 8 reanalyze workers used 3 million environment frames to reach an average return that takes the parameter set with 0 reanalyze workers 5 million environment frames.
The number of training targets used for training in the former is $3 * (\frac{20}{12}) = 5$ millions and this equals to latter 5 millions mark.
This means in the early stage of the training, each training target generated by the reanalyze workers contributes to the training as much as one training target generated by the training workers.
In the later stage of the training, all runs reached a similar final average return with the same amount of environment frames.
This means the training targets generated by the reanalyze workers do not contribute or can even harm the training process in the later stage.
We hypothesize the reason of this observation is that in the early stage of the training, training targets shift more quickly, and updating past trajectories with the latest model is more valuable.
However, in the later stage of the training, training targets stabilize and its more important to have better performing trajectories that more accurate estimates from worse trajectories.

\section{Analysis of Planning in \textit{Space Invaders}}
\includeimage{minatar_search}{\textbf{
        MooZi acting in \textit{Space Invaders} environment by planning with a learned model.
        \textit{Left}: The observation \(o_t\) and the agent's statistics.
        \textit{Right}: The search tree built by planning with the agent's learned model.
    }}{}
We show an example of \moozi acting in the \textit{Space Invaders} environment by planning with a learned model in Figure \ref{fig:minatar_search}.
The left hand side shows the latest environment observation $o_t$.
We also annotate agent statistics on top of the image.
The right hand side shows the search tree of the agent for deciding the next action.
Row \textit{R} shows the reward from the last timestep $r_t$.
The agent hit an enemy in the last timestep so $r_t = 1$.
Row \textit{V} shows the value prediction $v_t^{*}$ at the current timestep, which is also the value of the root node in the search tree on the right hand side.
Row \textit{P} shows the prior policy $\mathbf{p}^0_t$ before the tree search.
The prior policy is obtained by performing one initial inference described in Section \ref{sec:nn}.
Row \textit{N} hows the visit counts of nodes at the root.
\(\pi\) shows the posterior policy $\mathbf{p}^*_t$ after the tree search.
Row \textit{Q} shows the one-step returns of  \(\hat{r}_t^1 + \gamma v_t^1\).
Row \textit{A} shows the next action to take in the environment.
The prior policy shows that the agent favors two actions, \textit{Left} and \textit{Fire}.
The reason for going \textit{Left} is because the agent observes an enemy bullet traveling towards the agent.
The search reveals why \textit{Fire} is also a good idea: if the agent decides to \textit{Fire} at $t$, it could still dodge \textit{Left} at $t+1$.
The subtle difference is that by delaying the dodging one timestep, the agent hits the enemy one timestep earlier, and receives a greater discounted return.
The agent also learns to not dodge \textit{Right} in this situation, and the prior of this action is so low that it is not even explored in the search.
This is because the enemies are moving to the left, if the agent takes a \textit{Right}, subsequent \textit{Fire}s will miss.
After the search the posterior probability of taking the action \textit{Fire} is 96\%, which means the agent is going to take this action 96\% percent of the time.
The training target created from this interaction will also shift the prior of this action towards 96\% in future similar simulations.


\section{Learning through Self-play in \textit{Breakthrough}} \label{sec:exp:breakthrough}
\includeimage[0.9]{breakthrough}{
    \textbf{\textit{Left}: Breakthrough ($5 \times 5$) starting board. \textit{Right}: Breakthrough movement rule.}
}{}

We evaluate \moozi in the board game \textit{Breakthrough}.
\textit{Breakthrough} is played by two players, zero-sum, deterministic, and with perfect information for both players.
The original \textit{Breakthrough} is played on a $7 \times 7$ board, in our experiment we use smaller board size of \(5 \times 5\) and fewer pieces for both players.
Each piece can move forward, diagonally forward, or capture an enemy piece diagonally forward.
The player first reaches the home row of its opponent wins.
Figure \ref{fig:breakthrough} illustrates the starting board used in our experiment and the movement rules.
We use the OpenSpiel's implementation of \textit{Breakthrough}.
The action space size of the environment is around 300, and a typical state have five to twelve legal actions.

\includeimage[0.9]{breakthrough_elo}{\textbf{
        \moozi Evaluation of \moozi throughout training in \textit{Breakthrough}.
    }}
{
    The \textit{x} axis shows number of training steps of 23 model checkpoints.
    The \textit{y} axis shows the Elo rating of the checkpoints.
}
We train \moozi through self-play with a shared network for both players.
The planner is configured for two-players game as described in Section \ref{sec:planner}.
Figure \ref{fig:breakthrough_elo} shows the evaluation of \moozi throughout training.
We evaluate 23 model checkpoints by running a round-robin tournament in which all models pairs with all other models and play 16 matches alternating first and second player.
We initialize the Elo of each checkpoint to zero and compute the new Elo rating based on the results of the tournament.
The steady Elo rating increase shows the model learns reliably through self-play.

\section{Analysis of Planning in \textit{Breakthrough}}
\includeimage[0.7]{half_tree}
{
    \textbf{\moozi planning with a learned model in \textit{Breakthrough}.}}
{
    On nodes: \textit{I} means node index, \textit{R} means reward prediction \(\hat{r}\), \textit{V} means value prediction $v$, and \textit{N} means node visit count.
    On edges: \textit{A} means action index, \textit{p} means prior probability, \textit{Q} means $Q$ value.
}
Figure \ref{fig:half_tree} shows an example of MooZi planning in a two-players game using the learned model in \textit{Breakthrough}.
We use a game implementation as the perfect model, and rollout perfect game states using the actions sampled by the search to create a perfect game state tree.
We overlap the \moozi search tree and the perfect game state tree to create the visualization in the figure.
The learned model only knows the legal actions at the root level, so it is possible for the planner to create nodes using illegal actions beyond the root level.
When this happens, such nodes will not have their corresponding legal game states.
We call a search tree node without a corresponding legal game state a \textit{delusion}, and we color these nodes with light pink.
Node \textit{A} is a node in the search tree with its corresponding legal game state rendered as a board position.
Node \textit{B} is a \textit{delusion} that created by the learned model by taking action 41 that's not legal in the game state corresponding to node \textit{A}.
Node \textit{C} is not a \textit{delusion} because action 101 is one of the legal actions in the game state of node \textit{A}.
The search cannot different \textit{delusional} node from valid nodes, and the value predictions of node \textit{B} and node \textit{C} are both used to update their parent node \textit{A} through backpropagation.
One special case of a \textit{delusion} is when the search takes the dummy action after a terminal state.
Even though the dummy action is not a legal action in the game, it represents a valid prediction that the game ends.

\includeimage{full_tree}{
    \textbf{\moozi planning with a learned model that fully captures environment dynamics.}
}{
    The model accurately rollout game states from the starting state \textit{D} all the way to the terminal state \textit{E}.
    The red line is the cut-off of the training horizon, above which predictions are enforced by the training process.
}

In the early stage of training, the model frequently sample illegal actions beyond the root level, and the search trees are full of \textit{delusions} with arbitrary value and reward predictions.
As the model learns, the search trees become more accurate, and can fully capture environment dynamics.
Figure \ref{fig:full_tree} shows a search tree created using a well-learned model.
This model is trained with unroll steps $K = 5$ to predict value \(v\), reward \(\hat{r}\) and action distribution \(\mathbf{p}\) within a tree depth of five.
On top of the figure is the entire search tree with the root node \textit{D} as the top node.
The red line in the figure indicates the depth above which is within the training horizon and most nodes in the search tree are beyond the training horizon.
The planner uses the learned model to unroll from the starting state \textit{D} all the way to the terminal state \textit{E}, correctly predicts a black win with a reward of 1, and correctly predicts game termination by selecting the dummy action afterwards.
Moreover, this search tree satisfies all the following conditions: (1) every node is either associated with a legal game state, or correctly predicts game termination (2) every predicted reward is within 1\% margin of the actual reward of the associated game state (3) every node after a terminal node takes the dummy action and predicts a zero value and a zero reward.
This example shows that a well-learned model learns game dynamics far beyond five steps and can be used almost like a perfect model.

\section{Visualizing Hidden space of the Learned Representation}
We visualize the learned representation of MooZi by projecting hidden states into a lower dimension using dimension reduction techniques such as \textbf{principal component analysis (PCA)} and \textbf{t-distributed stochastic neighbor embedding (t-SNE)}.
We use one test worker to collect 20 trajectories in environment.