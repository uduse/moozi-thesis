\chapter{Introduction} \label{sec:intro}

\textbf{Deep Learning (DL)} is a branch of \textbf{Artificial Intelligence (AI)} that emphasizes the use of neural networks to fit any arbitrary function represented by a dataset.
The training of a neural network is done by computing a loss function from a batch of data, back-propagate gradients with respect to the loss, and updates weights and biases based on the gradients.
Deep learning techniques have been widely adopted in many domains, including computer vision, natural language processing, and robotics.

\textbf{Reinforcement Learning (RL)} is a branch of AI that emphasizes on solving decision making problems through trials and errors with delayed rewards.
RL had most success in the domain of \textbf{game playing}, in which the algorithm is represented as an \textbf{agent} and interacts with the game environments, such as boardgames and Atari games.
An extension to game playing is \textbf{general game playing (GGP)}, whose goal is to design a single agent that can play many different games without having much prior knowledge of the games.

\textbf{Deep Reinforcement Learning (DRL)} is a rising branch that combines DL and RL to solve decision making problems.
In a DRL system, the RL techniques lay out the structure of the algorithm such as the use the \textbf{agent-environment interface}, a value function, a reward signal, e.t.c., while the DL techniques are used approximate specific functions and learn representations.

\textbf{Planning} refers to any computational process that analyzes generated actions and their consequences in an environment.
In the RL terms, planning specifically means the use of a model to improve a policy.
In boardgames where perfect models are accessible, planning with these models yield great performance.
The most significant achievement of planning with a perfect model is AlphaGo beating human champion in Computer Go.
However, how to plan in games where no perfect models available remains a challenging problem to researchers.

A \textbf{distributed system} is a computer system that uses multiple processes with various purposes to complete tasks.
DRL systems for solving large problems are both data and compute intensive.
Utilizing concurrency to increase efficiency and throughput for these DRL systems are sometimes necessary.
Building a distributed system to achieve such concurrency is a common practice in the industry but requires significant engineering effort.

\section{Motivation}
\citeauthor{MasteringAtariGo_Schrittwieser.Antonoglou.ea_2020} developed MuZero, an algorithm that plans with a learned model (\ref{sec:muzero}).
This algorithm achieved the state-of-the-art in playing both Atari games and boardgames.
However, the source code of the algorithm is not publicly available, and the pseudo-code provided with the paper isn't sufficient to reproduce the full algorithm.
Moreover, MuZero requires much more computations than other model-free RL algorithms, and an inefficient implementation will drastically slows down experimentation.
We need an efficient and publicly available implementation of an algorithm that plans with a learned model.
This helps researchers understand how does the algorithm plan with its learned model, and facilitates future research.

\section{Contribution}
In this thesis we present the project \textbf{MooZi}, a system that play games by planning with a learned model.
This project includes:
\begin{itemize}
    \item A collection of environments bridges that connect the system to various common RL environments.
    \item Neural networks that learns representation and can be used for planning.
    \item A MCTS based planner that uses the learned model to perform planning.
    \item A distributed training system that efficiently trains the algorithm.
    \item A thesis with empirical studies and analysis.
\end{itemize}