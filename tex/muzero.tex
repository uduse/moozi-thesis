\subsection{MuZero} \label{sec:muzero}
In \citeyear{MasteringAtariGo_Schrittwieser.Antonoglou.ea_2020},
\citeauthor{MasteringAtariGo_Schrittwieser.Antonoglou.ea_2020} developed
\textbf{MuZero}, an algorithm that learns to play Atari, Go, chess and Shogi at superhuman level.
Compared to the AlphaGo faimily algorithms,
MuZero has less game specific knowledge and has no access to a perfect model.
MuZero plans with a neural network that learns the game dynamics through experience.
Since MuZero does not make the assumption of having access to a perfect model,
MuZero could be applied to games where either the perfect model is not known or is infeasible to compute with.

MuZero's model has three main functions.
The \textbf{representation function} $h$ encodes a history of observations $o_1, o_2, ..., o_t$ into a hidden state $s_t^0$.
The \textbf{dynamics function} $g$,
given a hidden state $s^k$ and action $a^k$, produces an immediate reward $r^k$ and the next hidden state $s^{k+1}$.
The \textbf{prediction function} $f$,
given a hidden state $s^k$, produces a probability distribution $p^k$ of actions and a value associated to that hidden state $v^k$.
These functions are approximated jointly in a neural network with weights $\theta$
\begin{align}
    x^0_t               & = h_{\theta}(o_1, o_2, ..., o_t) \label{eq:muzero_h}  \\
    (x^{k+1}, r^{k+1})  & = g_{\theta}(x^k, a^k)  \label{eq:muzero_g}  \\
    (v^k, \pmb{p}^k)    & = f_{\theta}(x^k) \label{eq:muzero_f}
\end{align}

MuZero plans with a search method based on the MCTS framework (discussed in \ref{sec:mcts}).
Due to the lack of access to a perfect model, MuZero's MCTS differs from a standard one in numerous ways.
The nodes are no longer perfect representations of the board states.
Instead, each node is associated with a hidden state $s$ as a learned representation of the board state.
The transition is no longer made by the perfect model but the dynamics function $g$.
Moreover, since the dynamics function also predicts a reward, edges created through inferencing with the dynamics function also contribute to the $Q$ value estimation.
% That is, traditionally we have $Q(s, a) = \mathit{E} \left[ V(S_{t + 1}) \mid S_t = s, A_t = a \right]$.

To act in the environment, MuZero plans following the MCTS framework described in section \ref{sec:mcts}.
At each timestep $t$, $s^0_t$ is created using (\ref{eq:muzero_h}).
% To select an action following the MCTS selection template equation , 
A variant of PUCT is used to select an action during the search
\begin{align*}
    I(s)     & = \argmax{a \in \mathcal{A}} \left( E(s, a) + B(s, a) \right)  \\
    E(s, a)  & = Q(s, a)  \\
    B(s, a)  & \propto P(s, a) \frac{\sqrt{N(s)}}{1+N(s, a)}\left[c_{1}+\log \left(\frac{N(s)+c_{2}+1}{c_{2}}\right)\right]
\end{align*}
where $c_1$ and $c_2$ are two constants that adjust the exploration bonus.
The selected edge $(s^k, a^k)$ at depth $k$ is expanded using (\ref{eq:muzero_g}) and evaluated using (\ref{eq:muzero_f}).
At the end of the simulation, the statistics of the nodes along the search path are updated.
Notice since the transitions of the nodes are approximated by the neural network, the search is performed over hypothetical trajectories without using a perfect model.
Finally, the action $a^0$ of the most visited edge $(s^0, a^0)$ of the root node is selected as the action to take in the environment.

Experience generated are stored in a replay buffer and processed to training targets.
The three functions of the model are trained jointly using the loss function
\begin{equation}
    \mathcal{L}_{t}(\theta)=
    \underbrace{\sum_{k=0}^{K} l^{\mathrm{p}}\left(\pi_{t+k}, p_{t}^{k}\right)}_{\circled{1}}
    +
    \underbrace{\sum_{k=0}^{K} l^{\mathrm{v}}\left(z_{t+k}, v_{t}^{k}\right)}_{\circled{2}}
    +
    \underbrace{\sum_{k=1}^{K} l^{\mathrm{r}}\left(u_{t+k}, r_{t}^{k}\right)}_{\circled{3}}
    +
    \underbrace{c\|\theta\|^{2}}_{\circled{4}}
\end{equation}
where $K$ is the number of rollout depth, \circled{1} is the loss of the predicted prior move probabilities and move probabilities imporved by the search, \circled{2} is the loss of the predicted value and experienced n-step return,
\circled{3} is the loss of the predicted reward and the experienced reward, and finally \circled{4} is the L2 regularization.

% In addition to the training samples generated by game play,
\textbf{MuZero Reanalyze} is also used to generate training targets in addition to those generated through game play.
MuZero Reanalyze re-executes MCTS on old games using the latest parameters and generates new training targets with potentially improved policy.