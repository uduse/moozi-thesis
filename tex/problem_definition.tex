\chapter{Problem Definition}

\section{Markov Decision Process and Agent-Environment Interface} \label{sec:markov}
A RL problem is usually represented as a \textbf{Markov Decision Process (MDP)}.
MDP is four-tuple $(\mathcal{S}, \mathcal{A}, R, P)$.
$\mathcal{S}$ is a set of states that forms the \textbf{state space}.
$\mathcal{A}$, is a set of actions that forms the \textbf{action space};
$P(s, a, s') = Pr[ s_{t+1} = s' \mid  s_t = s, a_t = a]$ is the \textbf{transition probability function}.
$R(s, a, s')$ is the \textbf{reward function}.
We use the \textbf{agent-environment interface} to solve a problem formulated as an MDP.
Figure \ref{fig:agent_environment_interface} illustrates the agent-environment interface.
The MDP is represented as the \textbf{environment}.
The decision maker that interacts with the environment is called the \textbf{agent}.
At each time step $t$, the agent starts at state $s_t \in \mathcal{S}$, takes an action $a_t \in \mathcal{A}$,
transitions to state $s_{t+1} \in \mathcal{S}$ based on the transition probability function $P(s_{t+1} \mid s_t, a_t)$
and receives a reward $R(S_t, A_t, S_{t+1})$.
These interactions yield a sequence of actions, states, and rewards $s_{0}, a_{0}, r_{1}, s_{1}, a_{1}, r_{2}, \dots$.
We call this sequence a \textbf{trajectory}.
When a trajectory ends at a terminal state $s_T$ at time $t = T$, this sequence is completed and we called it an \textbf{episode}.
Figure \ref{fig:agent_environment_interface} illustrates the interaction between the agent and the environment.

\includeimage[0.7]{agent_environment_interface}{
    \textbf{The Agent-Environment Interface, from \cite{ReinforcementLearningIntroduction_Sutton.Barto_2018}}.
}
\note{my notation is messed up but I will fix them later for sure.}


\section{Policies and Value Functions} \label{sec:policies_and_functions}
At each state $s$, then agent takes an action based on a \textbf{policy} $\pi(a \mid s)$.
This policy represents the conditional probability of the agent taking an action given a state $\pi(a \mid s) = Pr[ a_{t} = a \mid  s_t = s]$.
The agent maximizes the expected discounted sum of rewards from the current state $s_t$ following the policy $\pi$
\begin{align*}
    \label{eq:maximize_reward_undiscounted}
    \text{maximize} ~~~~  & \mathbb{E}_{\pi}\left[ G_t \mid s_t = s \right] , ~~~~ \forall s \in \mathcal{S}  \\
    G_t                   & = \sum_{k=0}^{T} \gamma^{k} r_{t+k+1} ~~~~ .
\end{align*}
Here $\gamma$ is the discount factor to favor short-term rewards.
$G$ is the discounted sum of rewards, or, equivalently, the discounted return.
We represent the maximization target above as the \textbf{value function} $V$ 
\begin{align*}
    V_\pi(s) = \mathbb{E}_{\pi}\left[ G_t \mid s_t = s \right] ~~.
\end{align*}
The value function tells us how good a state is following the policy $\pi$.
Similarly, we define the \textbf{state-action value function}
\begin{align*}
    Q_\pi(s, a) = \mathbb{E}_{\pi}\left[ G_t \mid s_t = s, a_t = a \right]  \\
\end{align*}
that tells us how good a state and action pair is.



% \note{also describes policy $\pi$}
% \note{address this is the most common formulation and how different libraries implement the interface}

% \subsection{Shortcomings of the Agent-Environment Interface for General Game Playing}
% % \note{I'm not sure if I should address these separatly.}
% \subsection{Multi-Agent Games}
% \note{address OpenSpiel's design multiple agents}
% \subsection{Partial Observability}
% \note{address POMDP}
% \subsection{Environment Stochasticity}
% \note{address OpenSpiel's design of random node}
% \subsection{Episodic vs Continuous}
% \note{barely seen in the literature, need more literature review}
% \subsection{Self-Observability}
% % \note{agent needs to be able to observe itself}
% \subsection{Environment Output Structure}
% % The agent-environment interface specifies two return types from the environment, namely the \textbf{observation} and the \textbf{reward}.
% % All environment implementations used in the RL field follow 